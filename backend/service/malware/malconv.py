import base64
import logging
from typing import Optional

from llama_cpp import Llama

from service.context import Context
from service.validator import Validator

logger = logging.getLogger(__name__)


class MalConvDetector(Validator):
    """
    Malware detection validator for email attachments.
    Uses llama_cpp with malware scanner model.
    """
    
    def __init__(self, repo_id: Optional[str] = None, filename: Optional[str] = None):
        """
        Initialize malware detector
        
        Args:
            repo_id: Hugging Face repo ID. Defaults to "yuvaliloo/malware-scanner-v1"
            filename: Model filename. Defaults to "llama-3-8b.Q4_K_M.gguf"
        """
        if repo_id is None:
            repo_id = "yuvaliloo/malware-scanner-v1"
        if filename is None:
            filename = "llama-3-8b.Q4_K_M.gguf"
        
        logger.info(f"Loading malware scanner model from {repo_id}/{filename}")
        self.model = Llama.from_pretrained(repo_id=repo_id, filename=filename)
        logger.info("Malware scanner model loaded successfully")
    
    def validate(self, context: Context) -> None:
        """
        Validate email attachments using malware scanner
        
        Args:
            context: Context containing the email request
        """
        if not context.email_request.attachments:
            logger.debug("No attachments to analyze")
            return
        
        logger.info(f"Analyzing {len(context.email_request.attachments)} attachment(s) with malware scanner")
        
        # Store results in context if needed
        if not hasattr(context, 'malware_detection_results'):
            context.malware_detection_results = []
        
        for attachment in context.email_request.attachments:
            try:
                result = self._analyze_attachment(attachment)
                context.malware_detection_results.append(result)
                logger.info(f"Attachment '{attachment.filename}': malicious={result['is_malicious']}, "
                          f"confidence={result['confidence']:.4f}")
            except Exception as e:
                logger.error(f"Error analyzing attachment '{attachment.filename}': {str(e)}", exc_info=True)
                context.malware_detection_results.append({
                    "filename": attachment.filename,
                    "error": str(e),
                    "is_malicious": False,
                    "confidence": 0.0
                })
    
    def _analyze_attachment(self, attachment) -> dict:
        """
        Analyze a single attachment using malware scanner LLM
        
        Args:
            attachment: AttachmentRequest object
            
        Returns:
            Dictionary with analysis results
        """
        if not attachment.bytes:
            logger.warning(f"Attachment '{attachment.filename}' has no bytes data")
            return {
                "filename": attachment.filename,
                "is_malicious": False,
                "confidence": 0.0,
                "error": "No bytes data provided"
            }
        
        try:
            # Decode base64 bytes
            file_bytes = base64.b64decode(attachment.bytes)
        except Exception as e:
            logger.error(f"Error decoding base64 for '{attachment.filename}': {str(e)}")
            return {
                "filename": attachment.filename,
                "is_malicious": False,
                "confidence": 0.0,
                "error": f"Base64 decode error: {str(e)}"
            }
        
        # Limit file size for LLM input (use first 64KB for analysis)
        max_bytes = 65536
        if len(file_bytes) > max_bytes:
            file_bytes = file_bytes[:max_bytes]
            logger.debug(f"File truncated to {max_bytes} bytes for analysis")
        
        # Convert bytes to hex string for LLM
        hex_string = file_bytes.hex()
        
        # Create prompt for malware detection
        prompt = f"Analyze this file for malware. Hex dump: {hex_string[:2000]}\n\nIs this file malicious? Respond with 'yes' or 'no'."
        
        # Get prediction from LLM
        output = self.model(prompt, max_tokens=10, echo=False)
        
        # Extract response text
        response_text = output['choices'][0]['text'].strip().lower()
        
        # Parse response to determine if malicious
        is_malicious = 'yes' in response_text or 'malicious' in response_text
        
        # Simple confidence based on response
        if is_malicious:
            confidence = 0.8 if 'yes' in response_text else 0.6
        else:
            confidence = 0.2 if 'no' in response_text else 0.5
        
        return {
            "filename": attachment.filename,
            "is_malicious": is_malicious,
            "confidence": confidence,
            "file_size": len(file_bytes),
            "response": response_text
        }

